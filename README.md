# ğŸ‘‹ Hi there! I'm **Vikas Malaviya**
ğŸ–¥ï¸ **Data Analyst | Data Engineer | Python | AWS | Spark | ETL Pipelines | Automation | Data Modeling**  
ğŸ” I specialize in building scalable, cloud-native data pipelines that transform raw data into actionable insights.

ğŸ“ Based in Melbourne, FL, USA  
ğŸ“ Masterâ€™s in Computer Science â€“ Florida Institute of Technology  

---

## âœ¨ About Me  
I am a Data Engineer with 5+ years of experience building scalable ETL/ELT pipelines, automating data workflows, and developing cloud-native data systems across AWS, Hadoop/Spark, and containerized environments.

Strong expertise in:  
- **Python**, **SQL**, **Pandas**, **Spark**, **Kafka**  
- **AWS (Lambda, S3, Redshift, DynamoDB, API Gateway, Step Functions, SQS)**  
- **Scraping automation** using Selenium & BeautifulSoup  
- **Containerization & Orchestration** with Docker & Kubernetes  
- **Data cleansing, modeling, and high-volume pipeline optimization**

Iâ€™m passionate about solving real business problems using clean, reliable, and automated data engineering systems.

---

## ğŸ”§ Skills  

### **Data Engineering**
- ETL/ELT Pipelines  
- Airflow / Step Functions  
- Data Modeling & Data Warehousing  
- API Integrations  

### **Programming**
- Python  
- SQL  
- Django / Flask  

### **Big Data & Cloud**
- AWS (Lambda, S3, Redshift, DynamoDB, API Gateway)  
- Apache Spark  
- Hadoop  
- Kafka  
- Docker & Kubernetes  

### **Databases**
- PostgreSQL  
- MongoDB  
- Cassandra  
- Redshift  

---

## ğŸ“Œ Featured Projects  

### ğŸ”¹ **1. End-to-End AWS ETL Pipeline (Python + Lambda + Redshift)**
A fully automated ETL pipeline built using AWS Lambda, S3, Redshift, and Step Functions.  
Processes both real-time and batch datasets with error-handling and performance monitoring.

**Tech:** Python, Lambda, Redshift, DynamoDB, Step Functions, S3  
**Features:** Auto-ingestion, data validation, transformation layer, incremental loading.

---

### ğŸ”¹ **2. Web Scraping Engine for Multi-Source Data Collection**
A modular scraping system for large-scale, scheduled extraction using Selenium and BeautifulSoup.

**Tech:** Python, Selenium, BeautifulSoup, Pandas  
**Features:** Proxy support, anti-bot handling, retry logic, structured output pipelines.

---

### ğŸ”¹ **3. Spark-Based Data Processing Pipeline**
A scalable data processing workflow for large datasets using Spark and Kafka, deployed via Docker/Kubernetes.

**Tech:** Apache Spark, Kafka, Docker, Kubernetes  
**Features:** Distributed processing, message streaming, high-throughput batch jobs.

---

### ğŸ”¹ **4. REST API Platform for Data Services (Django/Flask)**
Developed REST APIs enabling seamless data integrations and automated analytics pipelines.

**Tech:** Django, Flask, Python, PostgreSQL  
**Features:** Secure endpoints, JWT authentication, data ingestion endpoints.

---

## ğŸŒ Connect With Me  
ğŸ”— **LinkedIn:** https://linkedin.com/in/vikas-malaviya  
ğŸ“§ **Email:** vikasm2500@gmail.com  
ğŸ™ **GitHub:** @vikasmalaviya  

---

## ğŸ’» Tech Stack  
**Programming:** Python, SQL  
**Cloud:** AWS (Lambda, S3, Redshift, DynamoDB, API Gateway, Step Functions)  
**Big Data:** Spark, Hadoop, Kafka  
**Automation:** Airflow, Selenium  
**Databases:** PostgreSQL, MongoDB, Cassandra  
**DevOps:** Docker, Kubernetes  

---

â­ *Thanks for visiting! Feel free to explore my repositories or reach out for collaboration.*  
